{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ARAMTest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDMQcd3lsGNN"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLqhLGL14xp1"
      },
      "source": [
        "!pip install torchsummaryX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCoTDItg4u-h"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from torchsummaryX import summary\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
        "from torch.nn.utils import clip_grad_norm_, clip_grad_value_\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from multiprocessing import Pool"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_P1n58zlz3-"
      },
      "source": [
        "Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "172kK-H3LAPH"
      },
      "source": [
        "def fullprint(*args, **kwargs):\n",
        "  from pprint import pprint\n",
        "  import numpy\n",
        "  opt = numpy.get_printoptions()\n",
        "  numpy.set_printoptions(threshold=numpy.inf)\n",
        "  print(*args, **kwargs)\n",
        "  numpy.set_printoptions(**opt)\n",
        "\n",
        "def greedy_ordering(player_pools):\n",
        "  num_players, num_champions = player_pools.shape\n",
        "  champions = np.arange(1, num_champions + 1)\n",
        "  \n",
        "  universe_without_self = np.array([set() for _ in range(num_players)])\n",
        "  player_sets = []\n",
        "  for index, pool in enumerate(player_pools * champions):\n",
        "    player_pool = set(pool) - set([0])\n",
        "    player_sets.append(player_pool)\n",
        "    universe_without_self[list(set(list(range(num_players))) - set([index]))] |= player_pool\n",
        "\n",
        "  player_sets_excess_size = [ len(pool - pool.intersection(universe_without_self[index])) for index, pool in enumerate(player_sets) ]\n",
        "\n",
        "  return np.argsort(player_sets_excess_size)\n",
        "\n",
        "def hellinger_distance_loss(y_pred, y_true):\n",
        "  return (1/(2 ** 0.5)) * torch.mean(torch.sqrt(torch.sum(torch.square(torch.sqrt(y_pred) - torch.sqrt(y_true)), axis=2)))\n",
        "\n",
        "def closeness_to_uniform(y_pred, pools):\n",
        "  uniform = (pools / torch.unsqueeze(torch.sum(pools, axis=2), -1))\n",
        "  return hellinger_distance_loss(y_pred, uniform)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ISxl9ZelyU4"
      },
      "source": [
        "Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFIqzvBdHb51"
      },
      "source": [
        "N_CHAMPIONS = 152\n",
        "N_EPOCHS = 1000\n",
        "N_PLAYERS = 10\n",
        "SHARED_POOL_SIZE = 14\n",
        "BATCH_SIZE = 16\n",
        "SIMULATION_ITERS = 20000\n",
        "BASE_PATH = os.path.join('.', 'drive', 'My Drive')\n",
        "CHECKPOINT_PATH = os.path.join(BASE_PATH, 'ARAMProbabilityFunctionModel1.pt')\n",
        "DATA_TEST = os.path.join(BASE_PATH, 'data', 'test')\n",
        "DATA_VAL = os.path.join(BASE_PATH, 'data', 'val')\n",
        "DATA_TRAIN = os.path.join(BASE_PATH, 'data', 'train')\n",
        "TESTING = True\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQrYIBtalwWv"
      },
      "source": [
        "Define our datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E-ZIp_7oT1m"
      },
      "source": [
        "class PlayerChampionPools(IterableDataset):\n",
        "  def __init__(self, n_players=N_PLAYERS, n_champions=N_CHAMPIONS, shared_pool_size=SHARED_POOL_SIZE, simulation_iters=SIMULATION_ITERS):\n",
        "    self.n_players = n_players\n",
        "    self.n_champions = n_champions\n",
        "    self.shared_pool_size = shared_pool_size\n",
        "    self.simulation_iters = simulation_iters\n",
        "    self.cache_x = np.empty((0, self.n_players, self.n_champions), dtype=np.double)\n",
        "    self.cache_y = np.empty((0, self.n_players, self.n_champions), dtype=np.double)\n",
        "\n",
        "  def __iter__(self):\n",
        "    return self\n",
        "\n",
        "  def __next__(self):\n",
        "    print(\".\", end=\"\")\n",
        "    x = PlayerChampionPools.create_player_pools(self.n_players, self.n_champions, self.shared_pool_size)\n",
        "    y = PlayerChampionPools.simulate(self.simulation_iters, x)\n",
        "\n",
        "    self.cache_x = np.append(self.cache_x, np.expand_dims(x, 0), axis=0)\n",
        "    self.cache_y = np.append(self.cache_y, np.expand_dims(y, 0), axis=0)\n",
        "\n",
        "    if (self.cache_x.shape[0] >= BATCH_SIZE):\n",
        "      torch.save({\n",
        "          'batch_x': self.cache_x,\n",
        "          'batch_y': self.cache_y\n",
        "      }, os.path.join(DATA_TRAIN, 'batch-{}'.format(random.getrandbits(128)))) \n",
        "      self.cache_x = np.empty((0, self.n_players, self.n_champions))\n",
        "      self.cache_y = np.empty((0, self.n_players, self.n_champions))\n",
        "    \n",
        "    x_torch = torch.from_numpy(x).float().cuda()\n",
        "    y_torch = torch.from_numpy(y).float().cuda()\n",
        "\n",
        "    return (x_torch, y_torch)\n",
        "\n",
        "  '''\n",
        "  Using numpy here since torch is significantly slower (10x overhead).\n",
        "  I might be using it wrong, but the simulate function took upwards of 20 seconds \n",
        "  when using torch vs. 2 seconds when using numpy.\n",
        "  '''\n",
        "  @staticmethod\n",
        "  def create_player_pools(n_players, n_champions=N_CHAMPIONS, shared_pool_size=SHARED_POOL_SIZE):\n",
        "    shared_pool = np.random.choice(n_champions, shared_pool_size)\n",
        "\n",
        "    player_pools = np.empty(0, dtype=np.double)\n",
        "    for i in range(n_players):\n",
        "      player_pool = np.random.choice(n_champions, random.randint(0, n_champions))\n",
        "      player_final_pool = np.zeros(n_champions)\n",
        "      player_final_pool[np.concatenate((player_pool, shared_pool))] = 1\n",
        "      player_pools = np.concatenate((player_pools, player_final_pool))\n",
        "\n",
        "    player_pools = np.reshape(player_pools, (n_players, n_champions))\n",
        "\n",
        "    return player_pools\n",
        "\n",
        "  @staticmethod\n",
        "  def simulate(iters, player_pools):\n",
        "    num_champions = player_pools.shape[1]\n",
        "    dists = np.zeros_like(player_pools, dtype=np.double)\n",
        "    champions = np.arange(1, num_champions + 1)\n",
        "\n",
        "    for i in range(iters):\n",
        "      champions_selected = set()\n",
        "      for player_index, pool in enumerate(player_pools):\n",
        "        player_pool = set(pool * champions) - champions_selected - set([0])\n",
        "        player_selection = random.sample(player_pool, 1)[0]\n",
        "        dists[player_index, int(player_selection - 1)] += 1\n",
        "        champions_selected.add(player_selection)\n",
        "\n",
        "    return dists / np.expand_dims(np.sum(dists, axis=1), axis=-1)\n",
        "\n",
        "class SimulatedPools(Dataset):\n",
        "  def __init__(self, dataset_path):\n",
        "    self.dataset = [ os.path.join(dataset_path, filename) for filename in os.listdir(dataset_path) ]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    path = self.dataset[idx]\n",
        "    datapoint = torch.load(path)\n",
        "\n",
        "    x = torch.from_numpy(datapoint['batch_x']).squeeze().float()\n",
        "    y = torch.from_numpy(datapoint['batch_y']).squeeze().float()\n",
        "\n",
        "    return (x, y)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzjcNqeZltrA"
      },
      "source": [
        "Define our models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkoXmFdioZP3"
      },
      "source": [
        "class LSTMToLinearLayer(nn.Module):\n",
        "  '''\n",
        "  Input Size: (batch, n_players, n_champions)\n",
        "  Output Size: (batch, n_player, n_champions)\n",
        "  '''\n",
        "  def __init__(self, n_champions=N_CHAMPIONS, n_players=N_PLAYERS, is_last_layer=False, n_lstm_layers=1, lstm_dropout=0):\n",
        "    super(LSTMToLinearLayer, self).__init__()\n",
        "    self.n_champions = n_champions\n",
        "    self.n_players = n_players\n",
        "    self.is_last_layer = is_last_layer\n",
        "    self.lstm = nn.LSTM(\n",
        "        input_size=self.n_champions,\n",
        "        hidden_size=self.n_champions,\n",
        "        num_layers=n_lstm_layers,\n",
        "        batch_first=True,\n",
        "        dropout=lstm_dropout\n",
        "    )\n",
        "    self.lin = nn.Linear(self.n_players * self.n_champions, self.n_players * self.n_champions)\n",
        "    self.batchnorm = nn.BatchNorm1d(self.n_players)\n",
        "    self.output_layers = nn.Sequential(\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.BatchNorm1d(self.n_players)\n",
        "    )\n",
        "  \n",
        "  def forward(self, x_input):\n",
        "    x, _ = self.lstm(x_input)\n",
        "    x = self.batchnorm(x)\n",
        "    x = self.lin(torch.flatten(x, start_dim=1))\n",
        "    x = x.view((-1, self.n_players, self.n_champions))\n",
        "    if not self.is_last_layer:\n",
        "      x = self.output_layers(x)\n",
        "    return x\n",
        "\n",
        "class ProbabilityFunctionModel(nn.Module):\n",
        "  def __init__(self, n_champions=N_CHAMPIONS, n_players=N_PLAYERS):\n",
        "    super(ProbabilityFunctionModel, self).__init__()\n",
        "    self.n_champions = n_champions\n",
        "    self.n_players = n_players\n",
        "    self.network = nn.Sequential(\n",
        "        LSTMToLinearLayer(is_last_layer=True),\n",
        "        nn.Softmax(dim=2)\n",
        "    )\n",
        "\n",
        "  def forward(self, x_input):\n",
        "    return self.network(x_input)\n",
        "\n",
        "class ProbabilityFunctionModel2(nn.Module):\n",
        "  def __init__(self, n_champions=N_CHAMPIONS, n_players=N_PLAYERS):\n",
        "    super(ProbabilityFunctionModel2, self).__init__()\n",
        "    self.n_champions = n_champions\n",
        "    self.n_players = n_players\n",
        "    self.network = nn.Sequential(\n",
        "        LSTMToLinearLayer(n_lstm_layers=3, is_last_layer=True),\n",
        "        nn.Softmax(dim=2)\n",
        "    )\n",
        "\n",
        "  def forward(self, x_input):\n",
        "    return self.network(x_input)\n",
        "\n",
        "class ProbabilityFunctionModel3(nn.Module):\n",
        "  def __init__(self, n_champions=N_CHAMPIONS, n_players=N_PLAYERS):\n",
        "    super(ProbabilityFunctionModel3, self).__init__()\n",
        "    self.n_champions = n_champions\n",
        "    self.n_players = n_players\n",
        "    self.network = nn.Sequential(\n",
        "        LSTMToLinearLayer(),\n",
        "        nn.Dropout(),\n",
        "        LSTMToLinearLayer(),\n",
        "        nn.Dropout(),\n",
        "        LSTMToLinearLayer(is_last_layer=True),\n",
        "        nn.Softmax(dim=2)\n",
        "    )\n",
        "  \n",
        "  def forward(self, x_input):\n",
        "    return self.network(x_input)\n",
        "\n",
        "class ProbabilityFunctionModel4(nn.Module):\n",
        "  def __init__(self, n_champions=N_CHAMPIONS, n_players=N_PLAYERS):\n",
        "    super(ProbabilityFunctionModel4, self).__init__()\n",
        "    self.n_champions = n_champions\n",
        "    self.n_players = n_players\n",
        "    self.lstm = nn.LSTM(\n",
        "        input_size=self.n_champions,\n",
        "        hidden_size=self.n_champions,\n",
        "        num_layers=1,\n",
        "        batch_first=True,\n",
        "        dropout=0.5\n",
        "    )\n",
        "    self.batchnorm = nn.BatchNorm1d(self.n_players)\n",
        "    self.sequential = nn.Sequential(\n",
        "        nn.Linear(self.n_players * self.n_champions,  self.n_players * self.n_champions),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.BatchNorm1d(self.n_players * self.n_champions),\n",
        "        nn.Dropout(),\n",
        "        nn.Linear(self.n_players * self.n_champions,  self.n_players * self.n_champions), \n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.BatchNorm1d(self.n_players * self.n_champions),\n",
        "    )\n",
        "    self.softmax = nn.Softmax(dim=2)\n",
        "  \n",
        "  def forward(self, x_input):\n",
        "    x, _ = self.lstm(x_input)\n",
        "    x = self.batchnorm(x)\n",
        "    x = self.sequential(torch.flatten(x, start_dim=1))\n",
        "    x = x.view((-1, self.n_players, self.n_champions))\n",
        "    x = self.softmax(x)\n",
        "    return x\n",
        "\n",
        "class ProbabilityFunctionModel5(nn.Module):\n",
        "  def __init__(self, n_champions=N_CHAMPIONS, n_players=N_PLAYERS):\n",
        "    super(ProbabilityFunctionModel5, self).__init__()\n",
        "    self.n_champions = n_champions\n",
        "    self.n_players = n_players\n",
        "    self.lin1 = LSTMToLinearLayer()\n",
        "    self.lin2 = LSTMToLinearLayer()\n",
        "    self.lin3 = LSTMToLinearLayer()\n",
        "    self.lin4 = LSTMToLinearLayer(is_last_layer=True)\n",
        "    self.softmax = nn.Softmax(dim=2)\n",
        "  \n",
        "  def forward(self, x_input):\n",
        "    x1 = self.lin1(x_input)\n",
        "    x2 = self.lin2(x_input)\n",
        "    x3 = self.lin3(x_input)\n",
        "    \n",
        "    x4 = x1 * (x2 + x3)\n",
        "    x4 = self.lin4(x4)\n",
        "    x4 = self.softmax(x4)\n",
        "    return x4"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osSi67LZlrgN"
      },
      "source": [
        "Perform Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyPFD1cVNkZR"
      },
      "source": [
        "train_set = SimulatedPools(dataset_path=DATA_TRAIN)\n",
        "train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "val_set = SimulatedPools(dataset_path=DATA_VAL)\n",
        "val_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "test_set = SimulatedPools(dataset_path=DATA_TEST)\n",
        "test_dataloader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "model = ProbabilityFunctionModel5().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.25)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', threshold=0.01, patience=4, verbose=True)\n",
        "start_epoch = 0\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "test_losses = []\n",
        "best_loss = None\n",
        "\n",
        "if (os.path.exists(CHECKPOINT_PATH)):\n",
        "  checkpoint = torch.load(CHECKPOINT_PATH)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  start_epoch = checkpoint['end_epoch']\n",
        "  train_losses = checkpoint['train_losses']\n",
        "  val_losses = checkpoint['val_losses']\n",
        "  test_losses = checkpoint['test_losses']\n",
        "  best_loss = min([np.mean(loss) for loss in train_losses])\n",
        "  N_EPOCHS = N_EPOCHS - start_epoch\n",
        "\n",
        "summary(model, torch.randn((BATCH_SIZE, N_PLAYERS, N_CHAMPIONS)).to(device))\n",
        "\n",
        "torch.set_printoptions(profile=\"full\")\n",
        "for epoch in range(start_epoch + 1, start_epoch + N_EPOCHS + 1):\n",
        "  start = time.time()\n",
        "\n",
        "  print('Epoch {} of {}'.format(epoch, start_epoch + N_EPOCHS), end=\" \")\n",
        "\n",
        "  ## Training Dataset\n",
        "  running_train_losses = []\n",
        "  for batch_idx, batch in enumerate(train_dataloader):\n",
        "    print(\".\", end=\"\")\n",
        "    batch_x, batch_y = batch\n",
        "    batch_x = batch_x.to(device)\n",
        "    batch_y = batch_y.to(device)\n",
        "\n",
        "    dist_pred = model(batch_x)\n",
        "    loss = hellinger_distance_loss(dist_pred, batch_y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    clip_grad_norm_(model.parameters(), 0.05)\n",
        "    clip_grad_value_(model.parameters(), 0.005)\n",
        "    optimizer.step()\n",
        "\n",
        "    running_train_losses.append(loss.item())\n",
        "\n",
        "  train_losses.append(running_train_losses)\n",
        "  avg_train_loss = np.mean(running_train_losses)\n",
        "  scheduler.step(avg_train_loss)\n",
        "\n",
        "  ## Validation Dataset\n",
        "  running_val_losses = []\n",
        "  for batch_idx, batch in enumerate(val_dataloader):\n",
        "    batch_x, batch_y = batch\n",
        "    batch_x = batch_x.to(device)\n",
        "    batch_y = batch_y.to(device)\n",
        "\n",
        "    dist_pred = model(batch_x)\n",
        "    loss = hellinger_distance_loss(dist_pred, batch_y)\n",
        "    running_val_losses.append(loss.item())\n",
        "\n",
        "  val_losses.append(running_val_losses)\n",
        "  avg_val_loss = np.mean(running_val_losses)\n",
        "\n",
        "  ## Test Dataset\n",
        "  running_test_losses = []\n",
        "  for batch_idx, batch in enumerate(test_dataloader):\n",
        "    batch_x, batch_y = batch\n",
        "    batch_x = batch_x.to(device)\n",
        "    batch_y = batch_y.to(device)\n",
        "\n",
        "    dist_pred = model(batch_x)\n",
        "    loss = hellinger_distance_loss(dist_pred, batch_y)\n",
        "    running_test_losses.append(loss.item())\n",
        "\n",
        "  test_losses.append(running_test_losses)\n",
        "  avg_test_loss = np.mean(running_test_losses)\n",
        "\n",
        "  if not TESTING:\n",
        "    # save lowest loss\n",
        "    if (best_loss is None) or (avg_train_loss < best_loss):\n",
        "      torch.save({\n",
        "          'model_state_dict': model.state_dict(), \n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'end_epoch': epoch,\n",
        "          'train_losses': train_losses,\n",
        "          'val_losses': val_losses,\n",
        "          'test_losses': test_losses\n",
        "      }, CHECKPOINT_PATH)\n",
        "      best_loss = avg_train_loss\n",
        "\n",
        "    # save every couple of epochs\n",
        "    if (epoch % 50 == 0):\n",
        "      torch.save({\n",
        "          'model_state_dict': model.state_dict(), \n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'end_epoch': epoch, \n",
        "          'train_losses': train_losses,\n",
        "          'val_losses': val_losses,\n",
        "          'test_losses': test_losses\n",
        "      }, os.path.join(BASE_PATH, 'ARAMProbabilityFunctionModel1Epoch{}.pt'.format(epoch)))\n",
        "\n",
        "  print(\n",
        "      \" Avg Loss: {}, Avg Val Loss: {}, Avg Test Loss: {}, Best Loss: {}, Time Taken: {} seconds\"\n",
        "      .format(avg_train_loss, avg_val_loss, avg_test_loss, best_loss, time.time() - start))\n",
        "  \n",
        "torch.set_printoptions(profile=\"default\") # reset\n",
        "\n",
        "plt.figure(1, figsize=(20, 10))\n",
        "plt.plot(np.mean(train_losses, axis=1), label=\"Training Loss\", linewidth=3)\n",
        "plt.plot(np.mean(val_losses, axis=1), label=\"Validation Loss\", linewidth=3)\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.rcParams.update({'font.size': 22})\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nwnT7BMllkO"
      },
      "source": [
        "Simulate the problem using the neural network simulator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR95bSSI6QoK"
      },
      "source": [
        "model = ProbabilityFunctionModel().to(device)\n",
        "checkpoint = torch.load('/content/drive/MyDrive/WD 0.25, Batch Size 16/ARAMProbabilityFunctionModel1.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "diffs_random = []\n",
        "diffs_greedy = []\n",
        "\n",
        "for idx in range(1000000):\n",
        "  if idx % 10000 == 0:\n",
        "    print(idx)\n",
        "\n",
        "  pool = PlayerChampionPools.create_player_pools(10)\n",
        "  greedy_pool = torch.from_numpy(np.take(pool, greedy_ordering(pool), axis=0)).unsqueeze(0).float().to(device)\n",
        "  pool = torch.from_numpy(pool).unsqueeze(0).float().to(device)\n",
        "\n",
        "  rand_sim = model(pool)\n",
        "  diffs_random.append(closeness_to_uniform(rand_sim, pool).item())\n",
        "\n",
        "  greedy_sim = model(greedy_pool)\n",
        "  diffs_greedy.append(closeness_to_uniform(greedy_sim, greedy_pool).item())\n",
        "\n",
        "plt.figure(1, figsize=(20, 10))\n",
        "plt.hist([diffs_random, diffs_greedy], label=[\"random\", \"greedy\"])\n",
        "plt.xlabel('Hellinger Distance to Uniform Distribution')\n",
        "plt.ylabel('Number of Instances')\n",
        "plt.rcParams.update({'font.size': 22})\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"diffs random mean: \", np.mean(diffs_random))\n",
        "print(\"diffs greedy mean: \", np.mean(diffs_greedy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzebOJSglftg"
      },
      "source": [
        "Test out various architectures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBc8qjL9b2CW"
      },
      "source": [
        "model = ProbabilityFunctionModel().to(device)\n",
        "model2 = ProbabilityFunctionModel2().to(device)\n",
        "model3 = ProbabilityFunctionModel3().to(device)\n",
        "model4 = ProbabilityFunctionModel4().to(device)\n",
        "model5 = ProbabilityFunctionModel5().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "optimizer2 = torch.optim.Adam(model2.parameters())\n",
        "optimizer3 = torch.optim.Adam(model3.parameters())\n",
        "optimizer4 = torch.optim.Adam(model4.parameters())\n",
        "optimizer5 = torch.optim.Adam(model5.parameters())\n",
        "new_losses = []\n",
        "new_losses2 = []\n",
        "new_losses3 = []\n",
        "new_losses4 = []\n",
        "new_losses5 = []\n",
        "\n",
        "rand_input = torch.randn((BATCH_SIZE, N_PLAYERS, N_CHAMPIONS)).to(device)\n",
        "summary(model, rand_input)\n",
        "summary(model2, rand_input)\n",
        "summary(model3, rand_input)\n",
        "summary(model4, rand_input)\n",
        "summary(model5, rand_input)\n",
        "\n",
        "ds = SimulatedPools(dataset_path=DATA_VAL)\n",
        "dl = DataLoader(ds, batch_size=32, shuffle=True)\n",
        "\n",
        "for epoch in range(300):\n",
        "  for idx, batch in enumerate(dl):\n",
        "    (batch_x, batch_y) = batch\n",
        "    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "    dist_pred = model(batch_x)\n",
        "    loss = hellinger_distance_loss(dist_pred, batch_y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    clip_grad_norm_(model.parameters(), 0.05)\n",
        "    clip_grad_value_(model.parameters(), 0.005)\n",
        "    optimizer.step()\n",
        "    new_losses.append(loss.item())\n",
        "\n",
        "    dist_pred2 = model2(batch_x)\n",
        "    loss2 = hellinger_distance_loss(dist_pred2, batch_y)\n",
        "    optimizer2.zero_grad()\n",
        "    loss2.backward()\n",
        "    clip_grad_norm_(model2.parameters(), 0.05)\n",
        "    clip_grad_value_(model2.parameters(), 0.005)\n",
        "    optimizer2.step()\n",
        "    new_losses2.append(loss2.item())\n",
        "\n",
        "    dist_pred3 = model3(batch_x)\n",
        "    loss3 = hellinger_distance_loss(dist_pred3, batch_y)\n",
        "    optimizer3.zero_grad()\n",
        "    loss3.backward()\n",
        "    clip_grad_norm_(model3.parameters(), 0.05)\n",
        "    clip_grad_value_(model3.parameters(), 0.005)\n",
        "    optimizer3.step()\n",
        "    new_losses3.append(loss3.item())\n",
        "\n",
        "    dist_pred4 = model4(batch_x)\n",
        "    loss4 = hellinger_distance_loss(dist_pred4, batch_y)\n",
        "    optimizer4.zero_grad()\n",
        "    loss4.backward()\n",
        "    clip_grad_norm_(model4.parameters(), 0.05)\n",
        "    clip_grad_value_(model4.parameters(), 0.005)\n",
        "    optimizer4.step()\n",
        "    new_losses4.append(loss4.item())\n",
        "\n",
        "    dist_pred5 = model5(batch_x)\n",
        "    loss5 = hellinger_distance_loss(dist_pred5, batch_y)\n",
        "    optimizer5.zero_grad()\n",
        "    loss5.backward()\n",
        "    clip_grad_norm_(model5.parameters(), 0.05)\n",
        "    clip_grad_value_(model5.parameters(), 0.005)\n",
        "    optimizer5.step()\n",
        "    new_losses5.append(loss5.item())\n",
        "\n",
        "    print(\n",
        "        \"Epoch {}, loss1: {}, loss2: {}, loss3: {}, loss4: {}, loss5: {}\"\n",
        "        .format(epoch, loss.item(), loss2.item(), loss3.item(), loss4.item(), loss5.item()))\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(new_losses, label=\"ARAMNet\", linewidth=3)\n",
        "plt.plot(new_losses2, label=\"Network 2\", linewidth=3)\n",
        "plt.legend()\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.rcParams.update({'font.size': 22})\n",
        "plt.show()\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(new_losses, label=\"ARAMNet\", linewidth=3)\n",
        "plt.plot(new_losses3, label=\"Network 4\", linewidth=3)\n",
        "plt.legend()\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.rcParams.update({'font.size': 22})\n",
        "plt.show()\n",
        "\n",
        "plt.figure(3)\n",
        "plt.plot(new_losses, label=\"ARAMNet\", linewidth=3)\n",
        "plt.plot(new_losses4, label=\"Network 3\", linewidth=3)\n",
        "plt.legend()\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.rcParams.update({'font.size': 22})\n",
        "plt.show()\n",
        "\n",
        "plt.figure(4)\n",
        "plt.plot(new_losses, label=\"ARAMNet\", linewidth=3)\n",
        "plt.plot(new_losses5, label=\"Network 5\", linewidth=3)\n",
        "plt.legend()\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.rcParams.update({'font.size': 22})\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siGSNRiKlbRu"
      },
      "source": [
        "Generate Random Pools (and save them)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUAC8SLjQXPO"
      },
      "source": [
        "for i in range(500):\n",
        "  print(\"Start {} ...\".format(i), end=\" \")\n",
        "  start = time.time()\n",
        "  x = PlayerChampionPools.create_player_pools(10)\n",
        "  y = PlayerChampionPools.simulate(100000, x)\n",
        "  x = np.expand_dims(x, 0)\n",
        "  y = np.expand_dims(y, 0)\n",
        "  torch.save({ 'batch_x': x, 'batch_y': y}, os.path.join(DATA_TRAIN, 'batch-{}'.format(random.getrandbits(256))))\n",
        "  print(\"Completed in {} seconds\".format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}